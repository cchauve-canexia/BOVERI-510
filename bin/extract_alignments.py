#!/usr/bin/env python3
"""
Functions to analyze the results of a run of the indels pipeline on AWS
"""

# Standard imports
import argparse
import csv
import os
import shutil
import subprocess
import tarfile
from collections import defaultdict

# Local imports
from common_utils import (VCF_DUMP_FIELDS_SEP, VCF_DUMP_VALUES_SEP,
                          get_vcf_dump_file, read_input_log_file)
from smart_open import open

# Default S3 directory containing results
CCHAUVE_S3_OUTPUT = 'cchauve-orchestration-ch'

# Suffixes of files generated by the pipeline
INDELS_FILE_SUFFIX = '_indels_filtered_snpeff.vcf'
INDELS_FILE_SUFFIX_TGZ = f"{INDELS_FILE_SUFFIX}.tar.gz"
SNPS_FILE_SUFFIX = '_snps_filtered_snpeff.vcf'
SNPS_FILE_SUFFIX_TGZ = f"{SNPS_FILE_SUFFIX}.tar.gz"
MAIN_FILE_SUFFIX = '_main.tar.gz'
V_GRAPH_SUFFIX = '_variants_graph.txt'

# Keys to differentiate indels and SNPs
INDELS, SNPS = 'indels', 'snps'
CALLS_FILE_SUFFIX_TGZ = {
    INDELS: INDELS_FILE_SUFFIX_TGZ,
    SNPS: SNPS_FILE_SUFFIX_TGZ
}
CALLS_FILE_SUFFIX = {INDELS: INDELS_FILE_SUFFIX, SNPS: SNPS_FILE_SUFFIX}

ALIGNMENTS_HEADER = '# variant\talignments'

TMP_DIR_PREFIX = 'tmp'

# AWS cp command
AWS_CP = ['aws', 's3', 'cp']

MANIFESTS = [
    'CG001.v3.4_Amplicon_Manifest_Panel3.4.4_20170921.tsv',
    'CG001v4.0_Amplicon_Manifest_Panel4.0.3_20181101.tsv',
    'CG001v5.1_Amplicon_Manifest_Panel5.1.12_20200911.tsv'
]


def get_amplicons_coords():
    amplicons_coords = {}
    for manifest_file in MANIFESTS:
        manifest_path = os.path.join('assets', manifest_file)
        with open(manifest_path) as manifest:
            manifest_reader = csv.DictReader(manifest, delimiter='\t')
            for row in manifest_reader:
                amplicons_coords[row['Amplicon_ID']] = (row['Chr'],
                                                        int(row['Start']))
    return amplicons_coords


def out_dir(run_id, prefix):
    """
    Returns the path to the output directory for a run
    :param: run_id (str) run ID
    :param: prefix (str): prefix of the path

    :return: str: output directory path
    """
    return os.path.join(prefix, run_id)


def extract_variants(dump_file):
    """
    Exracts variant string and list of supporting amplicons from a dump file
    :param: dump_file (str): path to variants dump file
    :return: (list(str, str, list(str))): list of str version of variants
    to consider and for each the sample and list of amplicons it appears in
    """
    variants = []
    with open(dump_file) as variants_dump:
        variants_reader = csv.DictReader(variants_dump,
                                         delimiter=VCF_DUMP_FIELDS_SEP)
        for row in variants_reader:
            sample_id = row['sample']
            v_str = f"{row['chr']}:{row['pos']}:{row['ref']}:{row['alt']}"
            source = row['source'].split(VCF_DUMP_VALUES_SEP)
            variants.append((sample_id, v_str, source))
    return (variants)


def extract_main_files(run_id, sample_id_list, s3_bucket, tmp_dir, prefix='.'):
    """
    Reads and optionally dump indels VCF files of run run_id.
    :param: run_id (str): ID of the run
    :param: sample_id_list (list(str)): list of sample ID
    :param: s3_bucket (str): s3 bucket where to fetch the results
    :param: tmp_dir (str): prefix of tmp dir to extract files
    :param: prefix (str): prefix of the output directory
    """
    for sample_id in sample_id_list:
        main_file_name = f"{sample_id}{MAIN_FILE_SUFFIX}"
        main_file_path = os.path.join('s3://', s3_bucket, run_id,
                                      main_file_name)
        print(main_file_path)
        subprocess.call(AWS_CP + [main_file_path, '.'])
        tarfile.open(main_file_name, 'r:gz').extractall(path=tmp_run_dir)
        os.remove(main_file_name)


def extract_alignments(run_id, tmp_run_dir, dump_file, variants,
                       amplicons_coords):
    variants_split = defaultdict(list)
    for (sample_id, v_str, source) in variants:
        for amplicon_id in source:
            variants_split[(sample_id, amplicon_id)].append(v_str)
    out_dump = open(dump_file, 'a')
    for (sample_id, amplicon_id), v_str_list in variants_split.items():
        amplicon_chr = amplicons_coords[amplicon_id][0]
        amplicon_start = amplicons_coords[amplicon_id][1]
        # Reading variants graph
        v_graph_file = os.path.join(
            tmp_run_dir, f"{sample_id}_{amplicon_id}{V_GRAPH_SUFFIX}")
        v_graph_data = {}
        v_graph = open(v_graph_file, 'r').readlines()
        for variant in v_graph:
            variant_data = variant.rstrip().split('\t')
            v_str1 = variant_data[1].split(':')
            v_start = amplicon_start + int(v_str1[2])
            v_str = f"{amplicon_chr}:{v_start}:{v_str1[4]}:{v_str1[5]}"
            alignments = variant_data[2]
            v_graph_data[v_str] = alignments
        for v_str in v_str_list:
            v_str_split = v_str.split(':')
            chr = v_str_split[0]
            pos = v_str_split[1]
            ref = v_str_split[2]
            alt = v_str_split[3]
            out_str = (f"\n{sample_id}\t{chr}\t{pos}\t{ref}\t"
                       f"{alt}\t{amplicon_id}\t{v_graph_data[v_str]}")
            out_dump.write(out_str)
    out_dump.close()


if __name__ == "__main__":
    """
    Reads the input log from a set of runs and extracts for every indel the
    supporting alignments IDs

    Arguments:
    - input_log_file: input log file from a set of runs
    - output_dir: directory where the results are written
    - s3_bucket (optional, default cchauve-orchestration-ch): bucket where to
      fetch indels pipeline output files.
    """
    # Input file
    ARGS_RUNS_FILE = ['input_log_file', None, 'Input log file']
    # Results directory
    ARGS_OUTPUT_DIR = ['output_dir', None, 'Output directory']
    # S3 bucket containing the reuslts
    ARGS_S3_BUCKET = ['-s3', '--s3_bucket', 'S3 bucket containing the results']
    parser = argparse.ArgumentParser(
        description='Indels pipeline: analysis of results on AWS')
    parser.add_argument(ARGS_RUNS_FILE[0], type=str, help=ARGS_RUNS_FILE[2])
    parser.add_argument(ARGS_OUTPUT_DIR[0], type=str, help=ARGS_OUTPUT_DIR[2])
    parser.add_argument(ARGS_S3_BUCKET[0],
                        ARGS_S3_BUCKET[1],
                        default=CCHAUVE_S3_OUTPUT,
                        type=str,
                        help=ARGS_S3_BUCKET[2])
    args = parser.parse_args()

    amplicons_coords = get_amplicons_coords()
    (sample_id_lists, _) = read_input_log_file(args.input_log_file)
    prefix = args.output_dir
    out_dump_file = os.path.join(prefix, 'dump_alignments.tsv')
    out_dump = open(out_dump_file, 'w')
    out_dump.write('sample\tchr\tpos\tref\talt\tsource\talignments')
    out_dump.close()
    os.makedirs(TMP_DIR_PREFIX, exist_ok=True)
    for (run_id, run_name), sample_id_list in sample_id_lists.items():
        os.makedirs(out_dir(run_id, prefix), exist_ok=True)
        tmp_run_dir = os.path.join(TMP_DIR_PREFIX, run_id)
        os.makedirs(tmp_run_dir, exist_ok=True)
        # Extracting indel calls from dump file
        indels_dump_file = get_vcf_dump_file(run_id,
                                             prefix,
                                             INDELS,
                                             init=False)
        indels = extract_variants(indels_dump_file)
        extract_main_files(run_id,
                           sample_id_list,
                           args.s3_bucket,
                           tmp_run_dir,
                           prefix=prefix)
        extract_alignments(run_id, tmp_run_dir, out_dump_file, indels,
                           amplicons_coords)
        shutil.rmtree(tmp_run_dir)
